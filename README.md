# üåü Cuantizaci√≥n de Modelos en PyTorch: ¬°Reduciendo la Complejidad sin Sacrificar Precisi√≥n! üöÄ
## Introducci√≥n
La cuantizaci√≥n de modelos es una t√©cnica esencial en el campo del aprendizaje autom√°tico que permite reducir el tama√±o y la complejidad de los modelos sin sacrificar significativamente su precisi√≥n. En este README, exploraremos c√≥mo implementarla utilizando PyTorch, y aprenderemos a aplicar diferentes t√©cnicas de cuantizaci√≥n para mejorar el rendimiento de nuestros modelos. ¬°Prep√°rate para llevar tus modelos al siguiente nivel!

## T√©cnicas de Cuantizaci√≥n:

### üß† Cuantizaci√≥n Est√°tica
En esta t√©cnica, se analizan los datos de entrada y salida del modelo antes de la inferencia para determinar los rangos adecuados para la cuantizaci√≥n. Los pesos y las activaciones se cuantizan utilizando estos rangos predefinidos, permitiendo optimizaciones adicionales.
```
import torch
import torch.quantization

# Crear un modelo sencillo
class ModeloSencillo(torch.nn.Module):
    def __init__(self):
        super(ModeloSencillo, self).__init__()
        self.fc = torch.nn.Linear(10, 2)

    def forward(self, x):
        return self.fc(x)

# Instanciar el modelo y prepararlo para cuantizaci√≥n
model = ModeloSencillo()
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)

# Simular datos de calibraci√≥n
datos_calibracion = torch.randn(100, 10)
model(datos_calibracion)

# Cuantizar el modelo
torch.quantization.convert(model, inplace=True)

# Mostrar el modelo cuantizado
print(model)
```
### ‚ö° Cuantizaci√≥n Din√°mica
En esta t√©cnica, los pesos del modelo se cuantizan antes de la inferencia, pero las activaciones se cuantizan din√°micamente durante la inferencia. Esta t√©cnica es √∫til para modelos que se ejecutan en dispositivos con capacidades de c√≥mputo m√°s limitadas.
```
import torch.quantization

# Cuantizaci√≥n din√°mica del modelo
model_dynamic = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

# Mostrar el modelo cuantizado din√°micamente
print(model_dynamic)
```
### üî• Cuantizaci√≥n de Entrenamiento con Conciencia Cuantizada (QAT)
Esta t√©cnica ajusta el modelo durante el entrenamiento para que sea robusto a la cuantizaci√≥n. Aunque es m√°s compleja, puede resultar en modelos cuantizados con mejor desempe√±o.
```
!pip install torch

# Importamos las librer√≠as necesarias
import torch
import torch.nn as nn
import torch.quantization
from torch.utils.data import TensorDataset, DataLoader

# Definimos el dispositivo (GPU si est√° disponible, CPU en caso contrario)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Crear un modelo sencillo
class ModeloSencillo(nn.Module):
    def __init__(self):
        super(ModeloSencillo, self).__init__()
        self.fc = nn.Linear(10, 2)

    def forward(self, x):
        return self.fc(x)

# Instanciar el modelo y moverlo al dispositivo
model = ModeloSencillo().to(device)

# Preparar el modelo para QAT (Quantization Aware Training)
model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
torch.quantization.prepare_qat(model, inplace=True)

# Definir datos simulados de entrenamiento y moverlos al dispositivo
datos = torch.randn(100, 10).to(device)
etiquetas = torch.randint(0, 2, (100,)).to(device)
dataset = TensorDataset(datos, etiquetas)
dataloader = DataLoader(dataset, batch_size=10)

# Definir la funci√≥n de p√©rdida
criterion = nn.CrossEntropyLoss()

# Definir el optimizador
optim = torch.optim.SGD(model.parameters(), lr=0.01)

# Entrenar el modelo
for epoch in range(10):
    for datos, etiquetas in dataloader:
        optim.zero_grad()
        salida = model(datos)
        perdida = criterion(salida, etiquetas)
        perdida.backward()
        optim.step()

# Convertir a modelo cuantizado
torch.quantization.convert(model, inplace=True)

# Mostrar el modelo cuantizado
print(model)
```
## Conclusi√≥n
La cuantizaci√≥n de modelos es una t√©cnica poderosa que puede mejorar significativamente el rendimiento de tus modelos, especialmente en entornos con recursos limitados. Con PyTorch, implementar estas t√©cnicas es sencillo y eficiente. 

## üè∑Ô∏è Licencia
Este proyecto est√° bajo la licencia MIT. Consulte el archivo de LICENCIA para obtener m√°s detalles.
